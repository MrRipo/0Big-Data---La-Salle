{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#Variational AutoEncoder\n"],"metadata":{"id":"pdTQg6LwSLMb"}},{"cell_type":"markdown","source":["Basado en https://keras.io/examples/generative/vae/"],"metadata":{"id":"Xj4m6wfKSONZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oIp9erBgQ-Yq"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["# El primer paso será definir una función que reciba como parámetros las medias\n","# y los logaritmos de las varianzas de la distribución latente, y devuelva un tensor\n","# que representa las muestras obtenidas de la distribución latente.\n","\n","def sampling(z_mean, z_log_var):\n","    # Obtiene el tamaño del lote (batch) y la dimensión de z_mean\n","    lote = tf.shape(z_mean)[0]\n","    dim = tf.shape(z_mean)[1]\n","\n","    # Genera un tensor epsilon a partir de una distribución normal estándar\n","    epsilon = tf.keras.backend.random_normal(shape=(lote, dim))\n","\n","    # Realiza el muestreo utilizando la fórmula z = z_mean + exp(0.5 * z_log_var) * epsilon\n","    # Esto forma parte del proceso de reparametrización para entrenar la red de manera diferenciable\n","    return z_mean + tf.exp(0.5 * z_log_var) * epsilon"],"metadata":{"id":"xEKiUOHLRZRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Construimos el módulo ENCODER\n","# Definimos la dimensión latente del VAE\n","latent_dim = 2\n","\n","# Creamos las entradas del codificador, que son imágenes de tamaño 28x28 con un canal de escala de grises\n","encoder_inputs = keras.Input(shape=(28, 28, 1))\n","# Capa de convolución 2D con 32 filtros, tamaño del filtro 3x3, activación ReLU\n","x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n","# Capa de convolución 2D con 64 filtros, tamaño del filtro 3x3, activación ReLU\n","x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","# Capa de convolución 2D con 64 filtros, tamaño del filtro 3x3, activación ReLU\n","x = layers.Conv2D(64, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\n","# Aplicamos la capa Flatten para convertir el tensor a un vector 1D\n","x = layers.Flatten()(x)\n","# Capa totalmente conectada con 16 neuronas y activación ReLU\n","x = layers.Dense(16, activation=\"relu\")(x)\n","# Capa de salida para las medias de la distribución latente\n","z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n","# Capa de salida para los logaritmos de las varianzas de la distribución latente\n","z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n","# Utilizamos la función de muestreo definida previamente para obtener la muestra z de la distribución latente\n","z = sampling(z_mean, z_log_var)\n","# Creamos el modelo del codificador con las entradas y salidas definidas\n","encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n","encoder.summary()\n"],"metadata":{"id":"-VUO1jb5Rdc6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Construimos el módulo DECODER\n","\n","\n","# Creamos las entradas para el decodificador, que son muestras de la distribución latente\n","latent_inputs = keras.Input(shape=(latent_dim,))\n","# Capa totalmente conectada para expandir la dimensión\n","x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n","# Reshape para obtener un tensor 3D (7x7x64)\n","x = layers.Reshape((7, 7, 64))(x)\n","# Capa de convolución transpuesta para aumentar las dimensiones\n","x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","# Otra capa de convolución transpuesta\n","x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\n","# Capa de convolución transpuesta adicional\n","x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","# Capa de salida con una sola dimensión y activación sigmoidal para obtener valores entre 0 y 1\n","decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n","# Creamos el modelo del decodificador\n","decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n","decoder.summary()\n"],"metadata":{"id":"6C1yJcXjRh5h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Definimos qué hacer para cada paso de entrenamiento\n","def vae_train_step(data, encoder, decoder, optimizer):\n","    with tf.GradientTape() as tape:\n","        # Pasamos los datos por el modelo del codificador para obtener las estadísticas de la distribución latente\n","        z_mean, z_log_var, z = encoder(data)\n","\n","        # Generamos una reconstrucción a partir de las estadísticas obtenidas con el codificador\n","        reconstruction = decoder(z)\n","\n","        # Calculamos la pérdida de reconstrucción utilizando la binary cross-entropy\n","        reconstruction_loss = tf.reduce_mean(\n","            tf.reduce_sum(\n","                keras.losses.binary_crossentropy(data, reconstruction),\n","                axis=(1, 2)\n","            )\n","        )\n","\n","        # Calculamos la pérdida de regularización KL (Kullback-Leibler)\n","        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n","        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n","\n","\n","        # Calculamos la pérdida total sumando las dos pérdidas\n","        total_loss = reconstruction_loss + kl_loss\n","\n","    # Obtenemos los gradientes respecto a los pesos de los modelos del codificador y decodificador\n","    grads = tape.gradient(total_loss, encoder.trainable_weights + decoder.trainable_weights)\n","\n","    # Actualizamos los pesos de los modelos utilizando el optimizador\n","    optimizer.apply_gradients(zip(grads, encoder.trainable_weights + decoder.trainable_weights))\n","\n","    # Retorna un diccionario con las pérdidas calculadas\n","    return {\n","        \"loss\": total_loss,\n","        \"reconstruction_loss\": reconstruction_loss,\n","        \"kl_loss\": kl_loss,\n","    }"],"metadata":{"id":"dQspM3C9Rjfh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Leemos el dataset de dígitos (MNIST)\n","#Fijaros en que no estamos utilizando las labels, problema NO supervisado.\n","(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n","mnist_digits = np.concatenate([x_train, x_test], axis=0)\n","digits_escalados = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n","\n","#Entrenamos el modelo\n","from tqdm import tqdm\n","\n","EPOCHS = 40\n","BATCH_SIZE = 2048\n","optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n","\n","# Creamos el bucle de épocas\n","for epoch in range(EPOCHS):\n","    # Inicializamos una barra de progreso para ver cómo va el entrenamiento\n","    pbar = tqdm(total=digits_escalados.shape[0], desc=f'Época {epoch + 1}/{EPOCHS}', unit='lote')\n","\n","    # Inicializamos las pérdidas por época\n","    total_loss_epoch = 0.0\n","    reconstruction_loss_epoch = 0.0\n","    kl_loss_epoch = 0.0\n","\n","    # Iteramos sobre los lotes\n","    for i in range(0, digits_escalados.shape[0], BATCH_SIZE):\n","        # Entrenamos el VAE y obtenemos resultados\n","        train_results = vae_train_step(digits_escalados[i:i + BATCH_SIZE], encoder, decoder, optimizer)\n","\n","        # Actualizamos las pérdidas acumuladas por época\n","        total_loss_epoch += train_results[\"loss\"]\n","        reconstruction_loss_epoch += train_results[\"reconstruction_loss\"]\n","        kl_loss_epoch += train_results[\"kl_loss\"]\n","\n","        # Actualizamos la barra de progreso con el tamaño del lote\n","        pbar.update(BATCH_SIZE)\n","\n","        # Mostramos las pérdidas actuales\n","        pbar.set_postfix(pérdida_total=f'{total_loss_epoch / ((i // BATCH_SIZE) + 1):.4f}',\n","                         pérdida_reconstrucción=f'{reconstruction_loss_epoch / ((i // BATCH_SIZE) + 1):.4f}',\n","                         pérdida_kl=f'{kl_loss_epoch / ((i // BATCH_SIZE) + 1):.4f}')\n","\n","    # Cerramos la barra de progreso al final de la época\n","    pbar.close()\n","\n"],"metadata":{"id":"p1wAn1TQRpXg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Vamos a ver cómo han quedado distribuidas las muestras de nuestro dataset\n","#en el latent space.\n","def plot_label_clusters(data, labels):\n","    # display a 2D plot of the digit classes in the latent space\n","    z_mean, _, _ = encoder.predict(data)\n","    plt.figure(figsize=(12, 10))\n","    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n","    plt.colorbar()\n","    plt.xlabel(\"z[0]\")\n","    plt.ylabel(\"z[1]\")\n","    plt.show()\n","\n","\n","(x_train, y_train), _ = keras.datasets.mnist.load_data()\n","x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n","\n","plot_label_clusters(x_train, y_train)"],"metadata":{"id":"n6YPXthMR1Tu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Vamos a provar de generar dígitos.\n","#Haremos un grid.\n","def plot_latent_space(n=10, figsize=15):\n","    # display a n*n 2D manifold of digits\n","    digit_size = 28\n","    scale = 3 #Para escoger este valor, miramos la figura anterior.\n","    figure = np.zeros((digit_size * n, digit_size * n))\n","    # linearly spaced coordinates corresponding to the 2D plot\n","    # of digit classes in the latent space\n","    #Cogeremos n puntos que van desde -scale hasta scale.\n","    #Crearemos un array con el eje horizontal y otro con el vertical.\n","    grid_x = np.linspace(-scale, scale, n)\n","    grid_y = np.linspace(-scale, scale, n)[::-1]\n","\n","    for i, yi in enumerate(grid_y):\n","        for j, xi in enumerate(grid_x):\n","            z_sample = np.array([[xi, yi]])\n","            x_decoded = decoder.predict(z_sample)\n","            digit = x_decoded[0].reshape(digit_size, digit_size)\n","            figure[\n","                i * digit_size : (i + 1) * digit_size,\n","                j * digit_size : (j + 1) * digit_size,\n","            ] = digit\n","\n","    plt.figure(figsize=(figsize, figsize))\n","    start_range = digit_size // 2\n","    end_range = n * digit_size + start_range\n","    pixel_range = np.arange(start_range, end_range, digit_size)\n","    sample_range_x = np.round(grid_x, 1)\n","    sample_range_y = np.round(grid_y, 1)\n","    plt.xticks(pixel_range, sample_range_x)\n","    plt.yticks(pixel_range, sample_range_y)\n","    plt.xlabel(\"z[0]\")\n","    plt.ylabel(\"z[1]\")\n","    plt.imshow(figure, cmap=\"Greys_r\")\n","    plt.show()\n","\n","\n","plot_latent_space()"],"metadata":{"id":"Cg9l0C01RsR2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Ahora probad el código anterior utilizando el dataset de fashion mnist (el de los vestidos)."],"metadata":{"id":"EjYypLAhWV3e"},"execution_count":null,"outputs":[]}]}